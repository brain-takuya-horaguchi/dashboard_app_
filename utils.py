import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import json
import openai
import streamlit as st
import os
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

def validate_data(df: pd.DataFrame) -> Dict:
    """
    ãƒ‡ãƒ¼ã‚¿ã®å“è³ªã‚’æ¤œè¨¼ã™ã‚‹é–¢æ•°
    """
    validation_results = {
        'is_valid': True,
        'errors': [],
        'warnings': [],
        'statistics': {}
    }
    
    required_columns = [
        'æ±‚è·è€…ï¼šæ±‚è·è€…ID', 'ä¼æ¥­ï¼šä¼æ¥­å', 'é€²æ—ï¼šæ›¸é¡æå‡ºæ—¥', 
        'é€²æ—ï¼šé¢æ¥æ—¥', 'é€²æ—ï¼šé¢æ¥å›æ•°', 'é€²æ—ï¼šæœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°', 
        'é€²æ—ï¼šå†…å®šæ—¥', 'é€²æ—ï¼šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹'
    ]
    
    # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèª
    missing_columns = [col for col in required_columns if col not in df.columns]
    if missing_columns:
        validation_results['errors'].append(f"å¿…é ˆåˆ—ãŒä¸è¶³ã—ã¦ã„ã¾ã™: {', '.join(missing_columns)}")
        validation_results['is_valid'] = False
    
    if not validation_results['is_valid']:
        return validation_results
    
    # ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
    total_rows = len(df)
    
    # 1. ç©ºãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    if total_rows == 0:
        validation_results['errors'].append("ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
        validation_results['is_valid'] = False
        return validation_results
    
    # 2. é‡è¤‡ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
    duplicates = df.duplicated().sum()
    if duplicates > 0:
        validation_results['warnings'].append(f"é‡è¤‡è¡ŒãŒ {duplicates} ä»¶ã‚ã‚Šã¾ã™")
    
    # 3. æ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã®ç¢ºèª
    date_columns = ['é€²æ—ï¼šæ›¸é¡æå‡ºæ—¥', 'é€²æ—ï¼šé¢æ¥æ—¥', 'é€²æ—ï¼šå†…å®šæ—¥']
    for col in date_columns:
        invalid_dates = 0
        for val in df[col].dropna():
            try:
                pd.to_datetime(val)
            except:
                invalid_dates += 1
        
        if invalid_dates > 0:
            validation_results['warnings'].append(f"{col} ã«ç„¡åŠ¹ãªæ—¥ä»˜ãŒ {invalid_dates} ä»¶ã‚ã‚Šã¾ã™")
    
    # 4. é¢æ¥å›æ•°ã®ç¢ºèª
    invalid_interview_counts = df[df['é€²æ—ï¼šé¢æ¥å›æ•°'].notna() & (df['é€²æ—ï¼šé¢æ¥å›æ•°'] < 0)].shape[0]
    if invalid_interview_counts > 0:
        validation_results['warnings'].append(f"é¢æ¥å›æ•°ãŒè² ã®å€¤ã®è¡ŒãŒ {invalid_interview_counts} ä»¶ã‚ã‚Šã¾ã™")
    
    # 5. æœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°ã®ç¢ºèª
    invalid_final_flags = df[df['é€²æ—ï¼šæœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°'].notna() & 
                           (~df['é€²æ—ï¼šæœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°'].isin([0, 1]))].shape[0]
    if invalid_final_flags > 0:
        validation_results['warnings'].append(f"æœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°ãŒ0ã¾ãŸã¯1ä»¥å¤–ã®è¡ŒãŒ {invalid_final_flags} ä»¶ã‚ã‚Šã¾ã™")
    
    # çµ±è¨ˆæƒ…å ±
    validation_results['statistics'] = {
        'total_rows': total_rows,
        'unique_candidates': df['æ±‚è·è€…ï¼šæ±‚è·è€…ID'].nunique(),
        'unique_companies': df['ä¼æ¥­ï¼šä¼æ¥­å'].nunique(),
        'data_completeness': {
            'æ›¸é¡æå‡ºæ—¥': (df['é€²æ—ï¼šæ›¸é¡æå‡ºæ—¥'].notna().sum() / total_rows * 100),
            'é¢æ¥æ—¥': (df['é€²æ—ï¼šé¢æ¥æ—¥'].notna().sum() / total_rows * 100),
            'å†…å®šæ—¥': (df['é€²æ—ï¼šå†…å®šæ—¥'].notna().sum() / total_rows * 100)
        }
    }
    
    return validation_results


def generate_alerts(metrics_df: pd.DataFrame) -> List[Dict]:
    """
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æŒ‡æ¨™ã«åŸºã¥ã„ã¦ã‚¢ãƒ©ãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    alerts = []
    
    if metrics_df.empty:
        return alerts
    
    # 1. ä½å†…å®šç‡ã‚¢ãƒ©ãƒ¼ãƒˆ
    low_success_rate = metrics_df[metrics_df['å†…å®šç‡'] < 5]
    if not low_success_rate.empty:
        for _, company in low_success_rate.iterrows():
            alerts.append({
                'type': 'danger',
                'title': 'ğŸš¨ ç·Šæ€¥: æ¥µä½å†…å®šç‡',
                'company': company['ä¼æ¥­å'],
                'message': f"å†…å®šç‡ãŒ {company['å†…å®šç‡']:.1f}% ã¨æ¥µç«¯ã«ä½ã„çŠ¶æ³ã§ã™ã€‚å³åº§ã®å¯¾ç­–ãŒå¿…è¦ã§ã™ã€‚",
                'priority': 'high'
            })
    
    # 2. é•·æœŸå‡¦ç†æ™‚é–“ã‚¢ãƒ©ãƒ¼ãƒˆ
    slow_processing = metrics_df[metrics_df['å¹³å‡å‡¦ç†æ™‚é–“'] > 60]
    if not slow_processing.empty:
        for _, company in slow_processing.iterrows():
            alerts.append({
                'type': 'warning',
                'title': 'â° é•·æœŸå‡¦ç†æ™‚é–“',
                'company': company['ä¼æ¥­å'],
                'message': f"å¹³å‡å‡¦ç†æ™‚é–“ãŒ {company['å¹³å‡å‡¦ç†æ™‚é–“']} æ—¥ã¨é•·æœŸåŒ–ã—ã¦ã„ã¾ã™ã€‚ãƒ—ãƒ­ã‚»ã‚¹ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                'priority': 'medium'
            })
    
    # 3. æ›¸é¡é€šéç‡ç•°å¸¸ã‚¢ãƒ©ãƒ¼ãƒˆ
    low_document_rate = metrics_df[
        (metrics_df['æ›¸é¡æå‡ºæ•°'] > 5) & 
        (metrics_df['æ›¸é¡é€šéç‡'] < 10)
    ]
    if not low_document_rate.empty:
        for _, company in low_document_rate.iterrows():
            alerts.append({
                'type': 'warning',
                'title': 'ğŸ“„ æ›¸é¡é€šéç‡ä½ä¸‹',
                'company': company['ä¼æ¥­å'],
                'message': f"æ›¸é¡é€šéç‡ãŒ {company['æ›¸é¡é€šéç‡']:.1f}% ã¨ä½ä¸‹ã—ã¦ã„ã¾ã™ã€‚å¿œå‹Ÿæ›¸é¡ã®è³ªå‘ä¸ŠãŒå¿…è¦ã§ã™ã€‚",
                'priority': 'medium'
            })
    
    # 4. é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¢ãƒ©ãƒ¼ãƒˆï¼ˆè‰¯ã„ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
    high_performers = metrics_df[
        (metrics_df['å†…å®šç‡'] > 30) & 
        (metrics_df['å†…å®šæ•°'] > 2)
    ]
    if not high_performers.empty:
        for _, company in high_performers.iterrows():
            alerts.append({
                'type': 'success',
                'title': 'ğŸ‰ é«˜ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹',
                'company': company['ä¼æ¥­å'],
                'message': f"å†…å®šç‡ {company['å†…å®šç‡']:.1f}% ã®å„ªç§€ãªæˆæœã‚’è¨˜éŒ²ã—ã¦ã„ã¾ã™ã€‚ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä»–ç¤¾ã«ã‚‚å±•é–‹æ¤œè¨ã—ã¦ãã ã•ã„ã€‚",
                'priority': 'low'
            })
    
    # 5. ãƒ‡ãƒ¼ã‚¿ç•°å¸¸ã‚¢ãƒ©ãƒ¼ãƒˆ
    data_anomalies = metrics_df[
        (metrics_df['æ›¸é¡æå‡ºæ•°'] > 0) & 
        (metrics_df['1æ¬¡é¢æ¥æ•°'] > metrics_df['æ›¸é¡æå‡ºæ•°'])
    ]
    if not data_anomalies.empty:
        for _, company in data_anomalies.iterrows():
            alerts.append({
                'type': 'info',
                'title': 'ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç•°å¸¸',
                'company': company['ä¼æ¥­å'],
                'message': "1æ¬¡é¢æ¥æ•°ãŒæ›¸é¡æå‡ºæ•°ã‚’ä¸Šå›ã£ã¦ã„ã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªãŒå¿…è¦ã§ã™ã€‚",
                'priority': 'low'
            })
    
    # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
    priority_order = {'high': 0, 'medium': 1, 'low': 2}
    alerts.sort(key=lambda x: priority_order[x['priority']])
    
    return alerts


def generate_recommendations(metrics_df: pd.DataFrame) -> List[Dict]:
    """
    ãƒ‡ãƒ¼ã‚¿åˆ†æçµæœã«åŸºã¥ã„ã¦æ”¹å–„ææ¡ˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    recommendations = []
    
    if metrics_df.empty:
        return recommendations
    
    # 1. æ›¸é¡é¸è€ƒæ”¹å–„ææ¡ˆ
    low_doc_pass_rate = metrics_df[metrics_df['æ›¸é¡é€šéç‡'] < 20]
    if not low_doc_pass_rate.empty:
        recommendations.append({
            'category': 'æ›¸é¡é¸è€ƒ',
            'title': 'ğŸ“„ æ›¸é¡é¸è€ƒãƒ—ãƒ­ã‚»ã‚¹ã®æ”¹å–„',
            'description': 'æ›¸é¡é€šéç‡ãŒä½ã„ä¼æ¥­ãŒã‚ã‚Šã¾ã™ã€‚',
            'actions': [
                'å¿œå‹Ÿæ›¸é¡ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚’è¦‹ç›´ã™',
                'ä¼æ¥­ã®ãƒ‹ãƒ¼ã‚ºã«åˆã‚ã›ãŸæ›¸é¡ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º',
                'æ›¸é¡ä½œæˆç ”ä¿®ã®å®Ÿæ–½',
                'éå»ã®æˆåŠŸäº‹ä¾‹ã‚’åˆ†æã—ã¦å…±æœ‰'
            ],
            'target_companies': low_doc_pass_rate['ä¼æ¥­å'].tolist()
        })
    
    # 2. é¢æ¥å¯¾ç­–ææ¡ˆ
    low_interview_pass_rate = metrics_df[metrics_df['1æ¬¡é¢æ¥é€šéç‡'] < 30]
    if not low_interview_pass_rate.empty:
        recommendations.append({
            'category': 'é¢æ¥å¯¾ç­–',
            'title': 'ğŸ¯ é¢æ¥å¯¾ç­–ã®å¼·åŒ–',
            'description': '1æ¬¡é¢æ¥é€šéç‡ãŒä½ã„ä¼æ¥­ãŒã‚ã‚Šã¾ã™ã€‚',
            'actions': [
                'æ¨¡æ“¬é¢æ¥ã®å®Ÿæ–½',
                'ä¼æ¥­ç ”ç©¶ã®å¾¹åº•',
                'é¢æ¥å®˜ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†',
                'é¢æ¥ã‚¹ã‚­ãƒ«å‘ä¸Šç ”ä¿®ã®å®Ÿæ–½'
            ],
            'target_companies': low_interview_pass_rate['ä¼æ¥­å'].tolist()
        })
    
    # 3. å‡¦ç†æ™‚é–“çŸ­ç¸®ææ¡ˆ
    slow_companies = metrics_df[metrics_df['å¹³å‡å‡¦ç†æ™‚é–“'] > 45]
    if not slow_companies.empty:
        recommendations.append({
            'category': 'åŠ¹ç‡åŒ–',
            'title': 'âš¡ å‡¦ç†æ™‚é–“ã®çŸ­ç¸®',
            'description': 'é¸è€ƒãƒ—ãƒ­ã‚»ã‚¹ãŒé•·æœŸåŒ–ã—ã¦ã„ã‚‹ä¼æ¥­ãŒã‚ã‚Šã¾ã™ã€‚',
            'actions': [
                'ä¼æ¥­ã¨ã®å®šæœŸçš„ãªé€²æ—ç¢ºèª',
                'é¸è€ƒã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æœ€é©åŒ–',
                'ä¸­é–“ãƒ•ã‚©ãƒ­ãƒ¼ã‚¢ãƒƒãƒ—ã®å®Ÿæ–½',
                'ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã«ã‚ˆã‚‹åŠ¹ç‡åŒ–'
            ],
            'target_companies': slow_companies['ä¼æ¥­å'].tolist()
        })
    
    # 4. æˆåŠŸäº‹ä¾‹ã®æ¨ªå±•é–‹ææ¡ˆ
    top_performers = metrics_df.nlargest(3, 'å†…å®šç‡')
    if not top_performers.empty:
        recommendations.append({
            'category': 'æˆåŠŸäº‹ä¾‹',
            'title': 'ğŸ† æˆåŠŸäº‹ä¾‹ã®æ¨ªå±•é–‹',
            'description': 'é«˜ã„æˆæœã‚’å‡ºã—ã¦ã„ã‚‹ä¼æ¥­ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’ä»–ç¤¾ã«ã‚‚é©ç”¨ã§ãã¾ã™ã€‚',
            'actions': [
                'æˆåŠŸä¼æ¥­ã®ãƒ—ãƒ­ã‚»ã‚¹åˆ†æ',
                'ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ã®æ–‡æ›¸åŒ–',
                'ä»–ç¤¾ã¸ã®é©ç”¨å¯èƒ½æ€§æ¤œè¨',
                'æˆåŠŸè¦å› ã®å…±æœ‰ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Ÿæ–½'
            ],
            'target_companies': top_performers['ä¼æ¥­å'].tolist()
        })
    
    return recommendations


def calculate_conversion_funnel(df: pd.DataFrame, company_name: str = None) -> Dict:
    """
    ã‚³ãƒ³ãƒãƒ¼ã‚¸ãƒ§ãƒ³ãƒ•ã‚¡ãƒãƒ«ã‚’è¨ˆç®—ã™ã‚‹é–¢æ•°
    """
    if company_name:
        df = df[df['ä¼æ¥­ï¼šä¼æ¥­å'] == company_name]
    
    if df.empty:
        return {
            'funnel': {},
            'conversion_rates': {}
        }
    
    # å„æ®µéšã®æ•°ã‚’è¨ˆç®—
    funnel = {
        'æ¨è–¦': df['æ±‚è·è€…ï¼šæ±‚è·è€…ID'].nunique(),
        'æ›¸é¡æå‡º': df['é€²æ—ï¼šæ›¸é¡æå‡ºæ—¥'].notna().sum(),
        'æ›¸é¡é€šé': df['é€²æ—ï¼šé¢æ¥æ—¥'].notna().sum(),
        '1æ¬¡é¢æ¥': df[(df['é€²æ—ï¼šé¢æ¥å›æ•°'] >= 1) & (df['é€²æ—ï¼šé¢æ¥æ—¥'].notna())].shape[0],
        '2æ¬¡é¢æ¥ä»¥é™': df[(df['é€²æ—ï¼šé¢æ¥å›æ•°'] > 1) & (df['é€²æ—ï¼šé¢æ¥æ—¥'].notna())].shape[0],
        'æœ€çµ‚é¢æ¥': df[(df['é€²æ—ï¼šæœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°'] == 1) & (df['é€²æ—ï¼šé¢æ¥æ—¥'].notna())].shape[0],
        'å†…å®š': df['é€²æ—ï¼šå†…å®šæ—¥'].notna().sum()
    }
    
    # é€šéç‡ã‚’è¨ˆç®—ï¼ˆå„æ®µéšé–“ã®é€šéç‡ï¼‰
    conversion_rates = {}
    
    # å­˜åœ¨ã™ã‚‹æ®µéšã®ã¿ã‚’å¯¾è±¡ã«é€šéç‡ã‚’è¨ˆç®—
    stage_pairs = [
        ('æ¨è–¦', 'æ›¸é¡æå‡º'),
        ('æ›¸é¡æå‡º', 'æ›¸é¡é€šé'),
        ('æ›¸é¡é€šé', '1æ¬¡é¢æ¥'),
        ('1æ¬¡é¢æ¥', '2æ¬¡é¢æ¥ä»¥é™'),
        ('2æ¬¡é¢æ¥ä»¥é™', 'æœ€çµ‚é¢æ¥'),
        ('æœ€çµ‚é¢æ¥', 'å†…å®š')
    ]
    
    for from_stage, to_stage in stage_pairs:
        if from_stage in funnel and to_stage in funnel and funnel[from_stage] > 0:
            conversion_rates[f"{from_stage}â†’{to_stage}"] = (funnel[to_stage] / funnel[from_stage]) * 100
    
    # è¡¨ç¤ºç”¨ã«ãƒ•ã‚¡ãƒãƒ«ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰0ã®å€¤ã‚’å‰Šé™¤
    funnel = {k: v for k, v in funnel.items() if v > 0}
    
    return {
        'funnel': funnel,
        'conversion_rates': conversion_rates
    }


def export_summary_report(metrics_df: pd.DataFrame, alerts: List[Dict], recommendations: List[Dict]) -> str:
    """
    ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹é–¢æ•°
    """
    try:
        if metrics_df.empty:
            return "# ã‚¨ãƒ©ãƒ¼: ãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™\n\nåˆ†æå¯¾è±¡ã®ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # åŸºæœ¬çµ±è¨ˆã®è¨ˆç®—
        total_companies = len(metrics_df)
        total_recommendations = int(metrics_df['æ¨è–¦äººæ•°'].sum())
        total_applications = int(metrics_df['æ›¸é¡æå‡ºæ•°'].sum())
        total_offers = int(metrics_df['å†…å®šæ•°'].sum())
        
        # å†…å®šç‡ã®è¨ˆç®—ï¼ˆã‚¼ãƒ­é™¤ç®—ã‚’å›é¿ï¼‰
        overall_rate = (total_offers / total_recommendations * 100) if total_recommendations > 0 else 0
        
        report = f"""# ä¼æ¥­æ¡ç”¨åˆ†æãƒ¬ãƒãƒ¼ãƒˆ
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

## ğŸ“Š å…¨ä½“ã‚µãƒãƒªãƒ¼
- åˆ†æå¯¾è±¡ä¼æ¥­æ•°: {total_companies}
- ç·æ¨è–¦äººæ•°: {total_recommendations:,}
- ç·æ›¸é¡æå‡ºæ•°: {total_applications:,}
- ç·å†…å®šæ•°: {total_offers:,}
- å…¨ä½“å†…å®šç‡: {overall_rate:.1f}%

## ğŸ¯ ãƒˆãƒƒãƒ—ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ¼
"""
        
        # ä¸Šä½3ç¤¾ã®å‡¦ç†
        if len(metrics_df) > 0:
            top_3 = metrics_df.nlargest(min(3, len(metrics_df)), 'å†…å®šç‡')
            for i, (_, company) in enumerate(top_3.iterrows(), 1):
                report += f"{i}. {company['ä¼æ¥­å']}: å†…å®šç‡ {company['å†…å®šç‡']:.1f}%\n"
        else:
            report += "ãƒ‡ãƒ¼ã‚¿ãŒä¸è¶³ã—ã¦ã„ã¾ã™ã€‚\n"
        
        # ã‚¢ãƒ©ãƒ¼ãƒˆã®å‡¦ç†
        report += "\n## ğŸš¨ ã‚¢ãƒ©ãƒ¼ãƒˆ\n"
        if alerts:
            high_priority_alerts = [a for a in alerts if a.get('priority') == 'high']
            if high_priority_alerts:
                for alert in high_priority_alerts:
                    company = alert.get('company', 'ä¸æ˜')
                    title = alert.get('title', 'ä¸æ˜')
                    message = alert.get('message', 'ä¸æ˜')
                    report += f"- {title}: {company} - {message}\n"
            else:
                report += "- ç·Šæ€¥ã®ã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“\n"
        else:
            report += "- ã‚¢ãƒ©ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“\n"
        
        # æ”¹å–„ææ¡ˆã®å‡¦ç†
        report += "\n## ğŸ’¡ æ”¹å–„ææ¡ˆ\n"
        if recommendations:
            for rec in recommendations:
                title = rec.get('title', 'ä¸æ˜')
                description = rec.get('description', 'ä¸æ˜')
                actions = rec.get('actions', [])
                target_companies = rec.get('target_companies', [])
                
                report += f"### {title}\n"
                report += f"{description}\n"
                for action in actions:
                    report += f"- {action}\n"
                
                if target_companies:
                    companies_str = ', '.join(target_companies[:3])
                    if len(target_companies) > 3:
                        companies_str += f" ãªã© {len(target_companies)} ç¤¾"
                    report += f"å¯¾è±¡ä¼æ¥­: {companies_str}\n\n"
                else:
                    report += "å¯¾è±¡ä¼æ¥­: ãªã—\n\n"
        else:
            report += "- æ”¹å–„ææ¡ˆã¯ã‚ã‚Šã¾ã›ã‚“\n"
        
        # çµ±è¨ˆæƒ…å ±ã®è¿½åŠ 
        report += "\n## ğŸ“ˆ çµ±è¨ˆæƒ…å ±\n"
        if len(metrics_df) > 0:
            report += f"- æœ€é«˜å†…å®šç‡: {metrics_df['å†…å®šç‡'].max():.1f}%\n"
            report += f"- æœ€ä½å†…å®šç‡: {metrics_df['å†…å®šç‡'].min():.1f}%\n"
            report += f"- å¹³å‡å†…å®šç‡: {metrics_df['å†…å®šç‡'].mean():.1f}%\n"
            report += f"- å¹³å‡æ›¸é¡é€šéç‡: {metrics_df['æ›¸é¡é€šéç‡'].mean():.1f}%\n"
            report += f"- å¹³å‡å‡¦ç†æ™‚é–“: {metrics_df['å¹³å‡å‡¦ç†æ™‚é–“'].mean():.1f}æ—¥\n"
        
        return report
        
    except Exception as e:
        return f"# ã‚¨ãƒ©ãƒ¼: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã«å¤±æ•—ã—ã¾ã—ãŸ\n\nã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}"


def setup_openai_client():
    """OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’è¨­å®š"""
    try:
        # .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
        api_key = os.environ.get("OPENAI_API_KEY")
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Streamlit secretsã¾ãŸã¯session_stateã‹ã‚‰èª­ã¿è¾¼ã‚€
        if not api_key:
            api_key = st.secrets.get("OPENAI_API_KEY") or st.session_state.get("openai_api_key")
        
        if not api_key:
            return None
        
        client = openai.OpenAI(api_key=api_key)
        return client
    except Exception as e:
        st.error(f"OpenAI APIã®è¨­å®šã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}")
        return None


def create_data_summary(df: pd.DataFrame, metrics_df: pd.DataFrame) -> str:
    """ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã¦AIã«æä¾›"""
    try:
        # åŸºæœ¬çµ±è¨ˆ
        total_rows = len(df)
        unique_companies = df['ä¼æ¥­ï¼šä¼æ¥­å'].nunique()
        unique_candidates = df['æ±‚è·è€…ï¼šæ±‚è·è€…ID'].nunique()
        
        # ä¼æ¥­ä¸€è¦§ï¼ˆä¸Šä½10ç¤¾ï¼‰
        company_list = df['ä¼æ¥­ï¼šä¼æ¥­å'].value_counts().head(10).to_dict()
        
        # æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ã®è¦ç´„
        metrics_summary = ""
        if not metrics_df.empty:
            metrics_summary = f"""
æŒ‡æ¨™ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šä½5ç¤¾ï¼‰:
{metrics_df.head(5).to_string()}

å…¨ä½“çµ±è¨ˆ:
- ç·æ¨è–¦äººæ•°: {metrics_df['æ¨è–¦äººæ•°'].sum():,}
- ç·æ›¸é¡æå‡ºæ•°: {metrics_df['æ›¸é¡æå‡ºæ•°'].sum():,}
- ç·å†…å®šæ•°: {metrics_df['å†…å®šæ•°'].sum():,}
- å¹³å‡å†…å®šç‡: {metrics_df['å†…å®šç‡'].mean():.1f}%
- å¹³å‡æ›¸é¡é€šéç‡: {metrics_df['æ›¸é¡é€šéç‡'].mean():.1f}%
"""
        
        # æ—¥ä»˜ç¯„å›²
        date_columns = ['é€²æ—ï¼šæ›¸é¡æå‡ºæ—¥', 'é€²æ—ï¼šé¢æ¥æ—¥', 'é€²æ—ï¼šå†…å®šæ—¥']
        date_ranges = {}
        for col in date_columns:
            dates = pd.to_datetime(df[col], errors='coerce').dropna()
            if not dates.empty:
                date_ranges[col] = {
                    'min': dates.min().strftime('%Y-%m-%d'),
                    'max': dates.max().strftime('%Y-%m-%d')
                }
        
        summary = f"""
ãƒ‡ãƒ¼ã‚¿æ¦‚è¦:
- ç·è¡Œæ•°: {total_rows:,}
- ä¼æ¥­æ•°: {unique_companies:,}
- æ±‚è·è€…æ•°: {unique_candidates:,}

ä¸»è¦ä¼æ¥­ (å¿œå‹Ÿä»¶æ•°é †):
{json.dumps(company_list, ensure_ascii=False, indent=2)}

ãƒ‡ãƒ¼ã‚¿æœŸé–“:
{json.dumps(date_ranges, ensure_ascii=False, indent=2)}

{metrics_summary}

ãƒ‡ãƒ¼ã‚¿æ§‹é€ :
- å„è¡Œã¯æ±‚è·è€…ã¨ä¼æ¥­ã®çµ„ã¿åˆã‚ã›ã‚’è¡¨ã™
- é€²æ—ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã€æ›¸é¡æå‡ºæ—¥ã€é¢æ¥æ—¥ã€å†…å®šæ—¥ãªã©ã®æƒ…å ±ã‚’å«ã‚€
- é¢æ¥å›æ•°ã€æœ€çµ‚é¢æ¥ãƒ•ãƒ©ã‚°ãªã©ã®è©³ç´°æƒ…å ±ã‚‚å«ã‚€
"""
        
        return summary
    except Exception as e:
        return f"ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã®ä½œæˆã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def query_data_with_ai(question: str, df: pd.DataFrame, metrics_df: pd.DataFrame, client) -> str:
    """AIã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã«é–¢ã™ã‚‹è³ªå•ã«å›ç­”"""
    try:
        # ãƒ‡ãƒ¼ã‚¿ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆ
        data_summary = create_data_summary(df, metrics_df)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        system_prompt = f"""
ã‚ãªãŸã¯ä¼æ¥­æ¡ç”¨åˆ†æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è³ªå•ã«æ­£ç¢ºã§æœ‰ç”¨ãªå›ç­”ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚

{data_summary}

å›ç­”ã®éš›ã¯ä»¥ä¸‹ã®ç‚¹ã«æ³¨æ„ã—ã¦ãã ã•ã„:
1. ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸå…·ä½“çš„ãªæ•°å€¤ã‚’ä½¿ç”¨ã™ã‚‹
2. åˆ†æçµæœã‹ã‚‰å®Ÿç”¨çš„ãªæ´å¯Ÿã‚’æä¾›ã™ã‚‹
3. å¯èƒ½ãªå ´åˆã¯æ”¹å–„ææ¡ˆã‚‚å«ã‚ã‚‹
4. æ—¥æœ¬èªã§å›ç­”ã™ã‚‹
5. ä¸æ˜ãªç‚¹ãŒã‚ã‚Œã°ã€ãƒ‡ãƒ¼ã‚¿ã®åˆ¶ç´„ã‚’æ˜ç¢ºã«ã™ã‚‹
"""
        
        user_prompt = f"""
è³ªå•: {question}

ã“ã®è³ªå•ã«ã¤ã„ã¦ã€æä¾›ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰èª­ã¿å–ã‚Œã‚‹å…·ä½“çš„ãªæƒ…å ±ã‚„å‚¾å‘ã€ãã—ã¦å®Ÿç”¨çš„ãªæ´å¯Ÿã‚’å«ã‚ã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚
"""
        
        # OpenAI APIã‚’å‘¼ã³å‡ºã—
        response = client.chat.completions.create(
            model="gpt-4o-mini",  # gpt-4o-miniã‚’ä½¿ç”¨
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            max_tokens=1000,
            temperature=0.7
        )
        
        return response.choices[0].message.content
        
    except Exception as e:
        return f"AIåˆ†æã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {str(e)}"


def get_suggested_questions(df: pd.DataFrame, metrics_df: pd.DataFrame) -> List[str]:
    """ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦æ¨å¥¨è³ªå•ã‚’ç”Ÿæˆ"""
    suggestions = [
        "æœ€ã‚‚ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒè‰¯ã„ä¼æ¥­ã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
        "å†…å®šç‡ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹ã‚’æ•™ãˆã¦ãã ã•ã„",
        "æ›¸é¡é€šéç‡ãŒä½ã„ä¼æ¥­ã®ç‰¹å¾´ã¯ä½•ã§ã™ã‹ï¼Ÿ",
        "å‡¦ç†æ™‚é–“ãŒé•·ã„ä¼æ¥­ã«ã¤ã„ã¦åˆ†æã—ã¦ãã ã•ã„",
        "æœˆåˆ¥ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã«ã¤ã„ã¦æ•™ãˆã¦ãã ã•ã„",
        "ã©ã®ä¼æ¥­ã«æœ€ã‚‚åŠ›ã‚’å…¥ã‚Œã‚‹ã¹ãã§ã™ã‹ï¼Ÿ",
        "é¸è€ƒãƒ—ãƒ­ã‚»ã‚¹ã§æœ€ã‚‚èª²é¡Œã¨ãªã£ã¦ã„ã‚‹æ®µéšã¯ã©ã“ã§ã™ã‹ï¼Ÿ",
        "æˆåŠŸã—ã¦ã„ã‚‹ä¼æ¥­ã®å…±é€šç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ"
    ]
    
    # ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸå‹•çš„ãªè³ªå•ã‚’è¿½åŠ 
    if not metrics_df.empty:
        # æœ€ã‚‚å†…å®šç‡ã®é«˜ã„ä¼æ¥­
        best_company = metrics_df.loc[metrics_df['å†…å®šç‡'].idxmax(), 'ä¼æ¥­å']
        suggestions.append(f"{best_company}ãŒæˆåŠŸã—ã¦ã„ã‚‹ç†ç”±ã¯ä½•ã§ã™ã‹ï¼Ÿ")
        
        # æœ€ã‚‚å†…å®šç‡ã®ä½ã„ä¼æ¥­
        worst_company = metrics_df.loc[metrics_df['å†…å®šç‡'].idxmin(), 'ä¼æ¥­å']
        suggestions.append(f"{worst_company}ã®æ”¹å–„ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ")
    
    return suggestions


def save_chat_history(question: str, answer: str):
    """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ä¿å­˜"""
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []
    
    st.session_state.chat_history.append({
        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
        'question': question,
        'answer': answer
    })


def export_chat_history() -> str:
    """ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
    try:
        if 'chat_history' not in st.session_state or not st.session_state.chat_history:
            return "# ãƒãƒ£ãƒƒãƒˆå±¥æ­´\n\n**çŠ¶æ…‹**: å±¥æ­´ãŒã‚ã‚Šã¾ã›ã‚“\n\nè³ªå•ã‚’é€ä¿¡ã—ã¦ã‹ã‚‰ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚"
        
        export_text = f"""# ãƒãƒ£ãƒƒãƒˆå±¥æ­´
ç”Ÿæˆæ—¥æ™‚: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
å±¥æ­´ä»¶æ•°: {len(st.session_state.chat_history)} ä»¶

---

"""
        
        for i, chat in enumerate(st.session_state.chat_history, 1):
            timestamp = chat.get('timestamp', 'ä¸æ˜')
            question = chat.get('question', 'è³ªå•ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            answer = chat.get('answer', 'å›ç­”ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“')
            
            export_text += f"## è³ªå• {i} ({timestamp})\n\n"
            export_text += f"**ğŸ™‹â€â™€ï¸ è³ªå•:**\n{question}\n\n"
            export_text += f"**ğŸ¤– å›ç­”:**\n{answer}\n\n"
            export_text += "---\n\n"
        
        # çµ±è¨ˆæƒ…å ±ã®è¿½åŠ 
        export_text += f"""## ğŸ“Š çµ±è¨ˆæƒ…å ±
- ç·è³ªå•æ•°: {len(st.session_state.chat_history)}
- æœ€åˆã®è³ªå•: {st.session_state.chat_history[0].get('timestamp', 'ä¸æ˜')}
- æœ€å¾Œã®è³ªå•: {st.session_state.chat_history[-1].get('timestamp', 'ä¸æ˜')}
"""
        
        return export_text
        
    except Exception as e:
        return f"# ã‚¨ãƒ©ãƒ¼: ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã«å¤±æ•—ã—ã¾ã—ãŸ\n\nã‚¨ãƒ©ãƒ¼è©³ç´°: {str(e)}" 